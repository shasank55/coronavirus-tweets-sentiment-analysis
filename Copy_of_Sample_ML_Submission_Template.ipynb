{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shasank55/coronavirus-tweets-sentiment-analysis/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** - shasank chawla"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to build a classification model to predict the sentiment of COVID-19 tweets. The dataset contains information such as username, screen name, location, tweet timestamp, and the actual tweet text. Manual tagging has been done to label the sentiment of each tweet. The project involves preprocessing the text data, exploring and visualizing the dataset, feature engineering, and building and evaluating classification models using machine learning algorithms. The performance of the models will be evaluated based on metrics such as accuracy, precision, recall, and F1 score. The ultimate goal is to develop a model that accurately predicts the sentiment of COVID-19 tweets, which could be used to gain insights into public sentiment towards the pandemic."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Description\n",
        "This challenge asks you to build a classification model to predict the sentiment of COVID-19 tweets.The tweets have been pulled from Twitter and manual tagging has been done then.\n",
        "The names and usernames have been given codes to avoid any privacy concerns.\n",
        "The Features of The Dataset:\n",
        "\n",
        "1.UserName\n",
        "\n",
        "2.ScreenName\n",
        "\n",
        "3.Location\n",
        "\n",
        "4.TweetAt\n",
        "\n",
        "5.OriginalTweet\n",
        "\n",
        "6.Sentiment\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import package\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import ast\n",
        "import re # for regular expression operations \n",
        "import string\n",
        "from datetime import datetime as dt\n",
        "from datetime import date\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import matplotlib.style as style\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Sklearn Libraries\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from collections import Counter\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,roc_curve"
      ],
      "metadata": {
        "id": "3Nuu3VE1KSeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP Libraries\n",
        "import nltk # for text manipulation\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "a-dKm7hYKVml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jM_5MunyKnMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "tweets= pd.read_csv('/content/Coronavirus Tweets.csv',encoding='latin')"
      ],
      "metadata": {
        "id": "Zl5A-0WKKoKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "tweets.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "tweets.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "tweets.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "tweets.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(tweets.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my dataset we have 6 column and 41157 rows and there is no duplicates in dataset but we have null value in location column 8590 and after that data is analysis."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "tweets.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "tweets.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 41157 rows and 6 columns in our dataset. \n",
        "\n",
        "1.UserName = username of each user is coded username\n",
        "\n",
        "2.ScreenName = screen name is also coded screeen name\n",
        "\n",
        "3.Location = region of origin\n",
        "\n",
        "4.TweetAt = tweet timing\n",
        "\n",
        "5.OriginalTweet = first tweet in the thread\n",
        "\n",
        "6.Sentiment = sentiment of the tweet"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "tweets.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we don't need to delete the null values from the Location column Because we need to categorise the tweet based on the text in the OriginalTweet column. Additionally, we don't want to lose the important data in the dataset because might be affect the performance of our model**"
      ],
      "metadata": {
        "id": "T9lY8DDVx-oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the analysis of our dataset, it is crucial to understand the structure and contents of the data. To achieve this, we perform a series of data preparation steps, including the following:\n",
        "\n",
        "1.Identifying the columns: This step involves identifying the names of all the columns in the dataset.\n",
        "\n",
        "2.Describing the columns: In this step, we provide a brief description of each column, including its data type, range of values, and any other relevant information.\n",
        "\n",
        "3.Defining the variables: Next, we define the variables in the dataset, including their type (numerical, categorical, etc.), and their purpose.\n",
        "\n",
        "4.Checking unique values: This step involves checking the unique values in each column and determining if there are any duplicate values.\n",
        "\n",
        "5.Handling null values: we don't need to delete the null values from the Location column Because we need to categorise the tweet based on the text in the OriginalTweet column. Additionally, we don't want to lose the important data in the dataset.\n",
        "\n",
        "By following these steps, we can ensure that the dataset is properly strutural, and ready for analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#finding from which country the tweets most tweeted\n",
        "tweet_location=tweets['Location'].value_counts()\n",
        "print(tweet_location)\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 20 Location with highest tweets"
      ],
      "metadata": {
        "id": "o4WcIaB4uNPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 Location with highest tweets\n",
        "tweet_location=tweets['Location'].value_counts().reset_index().rename(columns = {'index':'Location','Location':'Tweet_no'})\n",
        "tweet_location=tweet_location.sort_values(by='Tweet_no',ascending=False).head(21)\n",
        "tweet_location"
      ],
      "metadata": {
        "id": "cuk-hWzwMf2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization Top 20 Location with highest tweets\n",
        "plt.figure(figsize=(15,7.5))\n",
        "sns.barplot(x=\"Location\", y=\"Tweet_no\", data=tweet_location)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top Location with highest tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8wu4vxRsR4c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot is used because it is an effective way to visualize the distribution of data and compare the values of different categories."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the chart show the locations with the highest number of tweets, which can give insight into the places where the topic being tweeted about is highly discussed or popular. The data can also be used to identify patterns or trends in terms of geographical distribution of tweets on a particular topic."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data show us from which location the highest top 20 tweets were tweeted so from this data later we do analysis from where postive negative or nutral tweets were tweeted thus on this basis we can create somne positive impact"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exploring the Sentiment Column**"
      ],
      "metadata": {
        "id": "GnGITcGHzTZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Count of tweets as per sentiment\n",
        "Sentiment=tweets['Sentiment'].value_counts().reset_index().rename(columns = {'index':'Sentiment Types','Sentiment':'Counts'})\n",
        "print(Sentiment)\n",
        "     "
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seaborn Bar plot Count of tweets as per sentiment\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=Sentiment, x=\"Sentiment Types\",y=\"Counts\",palette ='icefire_r')\n",
        "\n",
        "plt.title('Sentiments Types Tweet')"
      ],
      "metadata": {
        "id": "T-CIT-NYziEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this bar plot is used to visualize the distribution of sentiments based on the type of sentiment."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the sentiment trend of tweets on a topic, indicating the general public opinion. A positive sentiment suggests a positive opinion while a negative sentiment suggests a negative opinion. The chart helps to understand public feelings about the topic.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this bar plot can help create a positive business impact by analysing from where which type of  sentiment will come in future we can pridict on this basis."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**There are 5 subcategories in this case, so we will combine 5-class classification problem into a 3-class classification problem by replace Extremely Positive tweets with positive tweets and Extremely Negative tweets with negative tweets.**"
      ],
      "metadata": {
        "id": "LAuNh4cFzq_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Replacing these values 'Extremely Negative' : 'Negative', 'Extremely Positive' : 'Positive'\n",
        "replace_values = {\"Sentiment\": {'Extremely Negative' : 'Negative', 'Extremely Positive' : 'Positive'}}\n",
        "replacing_data = tweets.replace(replace_values)\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After the replacing count these sentiment values\n",
        "sentiment_count = replacing_data['Sentiment'].value_counts().reset_index()\n",
        "sentiment_count.columns=['Sentiment','count']\n",
        "sentiment_count"
      ],
      "metadata": {
        "id": "wrdlXflgGVh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the graph for the total number of tweets for the new 3 subcategories of the target variable\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Sentiment\", y= 'count', data=sentiment_count,palette='hsv')\n",
        "plt.xlabel(\"Sentiment\",fontsize=12)\n",
        "plt.ylabel(\"Count\",fontsize=12)"
      ],
      "metadata": {
        "id": "vNiPFTsuGctP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we choose bar plot because bar plot displays the values of a categorical variable in a very good way"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we just did the combine 5-class classification problem into a 3-class classification problem."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will combine 5-class classification problem into a 3-class classification problem by replace Extremely Positive tweets with positive tweets and Extremely Negative tweets with negative tweets. so this is good for furture uses in machine learning."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding from which loaction the highest number of negative sentiment tweets were tweeted**"
      ],
      "metadata": {
        "id": "Dk-8028IdzxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Filter the data to include only Negative tweets\n",
        "Negative_tweets = tweets[tweets[\"Sentiment\"] == \"Negative\"]\n",
        "\n",
        "# Count the number of Negative tweets from each location\n",
        "location_counts = Negative_tweets['Location'].value_counts()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_counts = Negative_tweets['Location'].value_counts().reset_index().rename(columns = {'index':'Location','Location':'Counts'})\n",
        "location_counts=location_counts.sort_values(by='Counts',ascending=False).head(21)\n",
        "location_counts"
      ],
      "metadata": {
        "id": "bS-iwPsi7pLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7.5))\n",
        "sns.barplot(data=location_counts, x=\"Counts\",y=\"Location\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top Location with Negative tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v7Nv0R_P8gkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots are a simple and intuitive way to display data, which makes them accessible to a wide range of audiences."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights we find from chats are from where the negative sentiments is coming\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes here we got the from where the negative sentiment is coming from taking this data we can pridict and it is used for future pridiction and it can crete the positive impact."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding which location the highest no of positive sentiments tweets were tweeted**"
      ],
      "metadata": {
        "id": "yA8sgJsUiT3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Filter the data to include only positive tweets\n",
        "positive_tweets = tweets[tweets[\"Sentiment\"] == \"Positive\"]\n",
        "\n",
        "# Count the number of positive tweets from each location\n",
        "location_counts = positive_tweets['Location'].value_counts()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_counts = positive_tweets['Location'].value_counts().reset_index().rename(columns = {'index':'Location','Location':'Counts'})\n",
        "location_counts=location_counts.sort_values(by='Counts',ascending=False).head(21)\n",
        "location_counts"
      ],
      "metadata": {
        "id": "2MZzFVOQvwy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7.5))\n",
        "sns.catplot(data=location_counts, x=\"Counts\",y=\"Location\", height=8)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top Location with positive tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r5OhW0G6zES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Catplot is highly customizable, which allows us to modify the appearance of our plots to better suit our needs."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the charts are that here we find the highest no of positive sentiment tweets from which loaction"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the highest no of postive sentiments tweets were tweeted from london uk and so on here we use this data to pridict in future and create some positive impact using it"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Filter the data to include only neutral tweets\n",
        "Neutral_tweets = tweets[tweets[\"Sentiment\"] == \"Neutral\"]\n",
        "\n",
        "# Count the number of positive tweets from each location\n",
        "location_counts = Neutral_tweets['Location'].value_counts()\n",
        "\n",
        "# Count the number of positive tweets from each location\n",
        "location_counts = Neutral_tweets['Location'].value_counts()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_counts = Neutral_tweets['Location'].value_counts().reset_index().rename(columns = {'index':'Location','Location':'Counts'})\n",
        "location_counts=location_counts.sort_values(by='Counts',ascending=False).head(21)\n",
        "location_counts"
      ],
      "metadata": {
        "id": "LYT7lv4lBc2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(location_counts['Location'], location_counts['Counts'], marker='o')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top Locations with Neutral Tweets')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Count')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqJ9SWTaDJqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line plots are simple and intuitive, which makes them easy to read and interpret for a wide range of audiences."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we find the neutral sentiment tweets are mostly coming from us,london,uk and so on"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have identified the locations from which the highest number of neutral tweets are originating. This information can be leveraged by businesses to potentially create a positive impact."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top dates with maximum number of tweets**"
      ],
      "metadata": {
        "id": "rB1F_cAoEvSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Top dates with maximum number of tweets\n",
        "Tweet_count=tweets['TweetAt'].value_counts().reset_index().rename(columns = {'index':'Tweet_Date'})\n",
        "Tweet_20=Tweet_count.sort_values(by='TweetAt',ascending=False).head(21)\n",
        "print(Tweet_20)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seaborn Bar plot Top dates with maximum number of tweets\n",
        "plt.figure(figsize=(16,8))\n",
        "sns.barplot(data=Tweet_20, x=\"TweetAt\",y=\"Tweet_Date\",palette = ('hsv'))\n",
        "plt.xticks(rotation=90)\n",
        "plt.fontsize=12\n",
        "plt.title('Top dates with maximum number of tweets')"
      ],
      "metadata": {
        "id": "hVyZePRgFXJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots can be customized to display a variety of information."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in our dataset we find that on which date the most number tweets were twetted"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot shows the top dates with the maximum number of tweets. The insights gained from this analysis could potentially help create a positive business impact, depending on the context and goals of the business."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Assuming your DataFrame is called \"tweets\"\n",
        "tweets['Month'] = pd.DatetimeIndex(tweets['TweetAt']).month"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tweets in each month\n",
        "month_counts = tweets['Month'].value_counts().reset_index().rename(columns={'index': 'month', 'Month': 'counts'})\n",
        "month_counts"
      ],
      "metadata": {
        "id": "U72caa8hHvx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_months = month_counts.sort_values(by='counts', ascending=False).head(11)\n",
        "print(top_10_months)"
      ],
      "metadata": {
        "id": "dBbcFclYHxC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13, 9))\n",
        "sns.barplot(data=top_10_months, x='month', y='counts')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Top 10 months with the highest number of tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HS1GroZdNSeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots are a simple and intuitive way to display data, which makes them accessible to a wide range of audiences."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the insights we get is in which month the highest number of tweets were tweeted."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in our dataset we can see that the highest number of tweeted is tweeted in which month from date time to month we convert it to see in a monthly basis and here we analize the dataset on it and pridict."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Convert the \"TweetAt\" column to a datetime object\n",
        "tweets['TweetAt'] = pd.to_datetime(tweets['TweetAt'])\n",
        "\n",
        "# Create a new column \"Month\" that contains the month of each tweet\n",
        "tweets['Month'] = tweets['TweetAt'].dt.month\n",
        "\n",
        "# Filter the tweets that have positive sentiment\n",
        "positive_tweets = tweets[tweets['Sentiment'] == 'Positive']\n",
        "\n",
        "# Count the number of positive tweets in each month\n",
        "positive_month_counts = positive_tweets['Month'].value_counts().reset_index().rename(columns={'index': 'Month', 'Month': 'Count'})\n",
        "\n",
        "# Sort the counts in descending order and select the top month\n",
        "top_month = positive_month_counts.sort_values(by='Count', ascending=False).iloc[0]\n",
        "\n",
        "print('The month with the highest number of positive tweets is:', top_month['Month'])"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the number of positive tweets by month on a line chart\n",
        "sns.barplot(data=positive_month_counts, x='Month', y='Count')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of positive tweets')\n",
        "plt.title('Number of positive tweets by month')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Avvmu-kQhJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots are a simple and intuitive way to display data, which makes them accessible to a wide range of audiences."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are in which month we have the highest number of positive sentiment tweets."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are in which month we have the highest number of positive sentiment tweets and from this data we can use it in ml algorithim to pridict the furture sentiments\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#Filter the tweets that have negative sentiment\n",
        "negative_tweets = tweets[tweets['Sentiment'] == 'Negative']\n",
        "\n",
        "# Count the number of negative tweets in each location\n",
        "negative_location_counts = negative_tweets['Location'].value_counts().reset_index().rename(columns={'index': 'Location', 'Location': 'Count'})\n",
        "\n",
        "# Sort the counts in descending order and select the top location\n",
        "top_location = negative_location_counts.sort_values(by='Count', ascending=False).iloc[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter the tweets that have negative sentiment\n",
        "negative_tweets = tweets[tweets['Sentiment'] == 'Negative']\n",
        "\n",
        "# Count the number of negative tweets in each location\n",
        "negative_location_counts = negative_tweets['Location'].value_counts().reset_index().rename(columns={'index': 'Location', 'Location': 'Count'})\n",
        "\n",
        "# Sort the counts in descending order and select the top 20 locations\n",
        "top_locations = negative_location_counts.sort_values(by='Count', ascending=False).head(20)\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(15,7.5))\n",
        "sns.barplot(data=top_locations, x='Count', y='Location')\n",
        "plt.xlabel('Number of negative tweets')\n",
        "plt.ylabel('Location')\n",
        "plt.title('Top 20 locations with the highest number of negative sentiments')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3odCWFq7WAG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots make it easy to compare the values of different categories or group"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight we found that from which location the maximum number  of negative tweets were tweeted."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we gained the insight from the chart is that the maximum number of negative tweets were tweeted from london uk an sop on this information we can use to pridict the future sentiment."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "tweet_sentiment=tweets['Sentiment'].value_counts().reset_index().rename(columns = {'index':'Sentiment','Sentiment':'Number_of_Tweets'})\n",
        "tweet_sentiment=tweet_sentiment.sort_values(by='Number_of_Tweets',ascending=False)"
      ],
      "metadata": {
        "id": "lrPwYImNGpaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pie chart for percentage distribution of sentiment of tweets on old data.\n",
        "explode = (0, 0.2, 0,0.1,0)\n",
        "labels = 'Positive', 'Negative', 'Neutral','positive_sentiment','negative_sentiment'\n",
        "wp = { 'linewidth' : 1, 'edgecolor' : \"green\" }\n",
        "fig, ax = plt.subplots(figsize =(15, 10))\n",
        "wedges, texts, autotexts=ax.pie(data=tweet_sentiment,x='Number_of_Tweets',labels=labels,autopct='%1.1f%%',shadow=True,explode=explode,colors=['navajowhite','aquamarine','lightblue','orange','bisque'],wedgeprops = wp,textprops = dict(color =\"magenta\"))\n",
        "plt.setp(autotexts, size = 15, weight =\"bold\")\n",
        "plt.legend( loc=1)\n",
        "ax.set_title(\"Sentiment Distribution\");"
      ],
      "metadata": {
        "id": "1fcRPxC6WWyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are useful for showing the proportion of different categories within a data set. The size of each segment is proportional to the percentage of data it represents."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are from pie chart is we did  sentiment Distribution and divide all the sentiments into percentage term"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the maximum percentage of sentimental distribution is positive sentiments and later we can see the negative sentiments comes and so on we can use this for pridiction."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# function for hashtags collection \n",
        "def hashtag(x):\n",
        "    ''' This function collects all\n",
        "      the hashtags from the tweets '''\n",
        "    \n",
        "    hashtags_list = []\n",
        "    \n",
        "    for i in x:\n",
        "        ht = re.findall(r'#(\\w+)', i)\n",
        "        for item in ht:\n",
        "          hashtags_list.append(item)\n",
        "    \n",
        "    return hashtags_list\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hashtags=hashtag(tweets['OriginalTweet'])"
      ],
      "metadata": {
        "id": "MT4PvBYLXo5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hashtags_df=pd.DataFrame({\"Hashtags\":total_hashtags})\n",
        "total_hashtags_df"
      ],
      "metadata": {
        "id": "KXtWt7dlXtvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of total hashtags\n",
        "len(total_hashtags_df)"
      ],
      "metadata": {
        "id": "iG3NeRsxX5SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unique hash tags\n",
        "total_hashtags_df.nunique()"
      ],
      "metadata": {
        "id": "3C46mkKsX7YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_hashtags=total_hashtags_df.value_counts().sort_values(ascending=False).reset_index().rename({0:'Number_of_Hashtags'},axis=1).head(50)\n",
        "top_hashtags"
      ],
      "metadata": {
        "id": "n4VSQBD-YI7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(17,6))\n",
        "sns.barplot(data=top_hashtags, x=\"Hashtags\", y=\"Number_of_Hashtags\")\n",
        "plt.xticks(fontsize=15,rotation=90);\n",
        "plt.xlabel(\"HashTags\",fontsize=20);\n",
        "plt.yticks(fontsize=10);\n",
        "plt.ylabel(\"Number_of_Hashtags\",fontsize=20);\n",
        "plt.title(\"Top 50 hashtags\")"
      ],
      "metadata": {
        "id": "5bCbTBlRYMq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating a word cloud to see frequently occurring words in tweets**"
      ],
      "metadata": {
        "id": "qwuX7KKGSycd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After removing punctuation, stopwords and usnig stemming to words cropped to save space from the tweets we want to know which words are mostly used by the people.**"
      ],
      "metadata": {
        "id": "CWOkTe6ATBDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a text of words present in all the tweets\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "all_words = ' '.join([str(text) for text in tweets['OriginalTweet']])\n",
        "wordcloud = WordCloud(width=800, \n",
        "                      height=500, \n",
        "                      stopwords=set(STOPWORDS),\n",
        "                      background_color=\"white\", \n",
        "                      random_state=21, \n",
        "                      max_font_size=110).generate(all_words)\n",
        "# Displaying the generated word cloud\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XeHEVFVwdL-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplots can be used to visualize trends over time or across different conditions and A word cloud is a graphical representation of text data, where the most frequent words in a text corpus are displayed in a visual format."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are from the chart is we find the top 50 most used hathtags and we used the word cloud to find the max word used in the tweets"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are from the chart is we find the top 50 most used hathtags and we find that the highest coronavirus and covid19 and so on this will used to pridict and in the word cloud we can see the most frequent use words this will help to gain insights from tweets to create positive impact."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Get a list of all hashtags from the tweets\n",
        "hashtags_list = []\n",
        "for tweet in tweets['OriginalTweet']:\n",
        "    hashtags = re.findall(r'#(\\w+)', tweet)\n",
        "    hashtags_list.extend(hashtags)\n",
        "\n",
        "# Create a new DataFrame to count the number of hashtags per location\n",
        "location_hashtags = pd.DataFrame({'Count': tweets.groupby('Location')['OriginalTweet'].apply(lambda x: sum(x.str.count('#\\w+')))})\n",
        "\n",
        "# Get the location with the highest number of hashtags\n",
        "top_location = location_hashtags.idxmax()[0]\n",
        "\n",
        "print(\"The location with the highest number of hashtags is:\", top_location)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Define a list of colors\n",
        "colors = ['#FF5733', '#FFC300', '#C70039', '#900C3F', '#581845', '#2ECC71', '#27AE60', '#3498DB', '#2980B9', '#8E44AD']\n",
        "\n",
        "# Create a barplot of the top locations\n",
        "plt.bar(top_locations.index, top_locations['Count'], color=colors)\n",
        "plt.xticks(rotation=90, fontsize=12, fontweight='bold')\n",
        "plt.yticks(fontsize=12, fontweight='bold')\n",
        "plt.xlabel('Location', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Number of Hashtags', fontsize=14, fontweight='bold')\n",
        "plt.title('Top 10 Locations with the Highest Number of Hashtags', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4CHUr1HbOvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are useful for visualizing a small number of categories, typically up to 10 or 20. If there are many categories, a bar plot may become overcrowded and difficult to interpret."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are that we see that the highest number of hashtag are from which location tweeted."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights are that we see that the highest number of hashtag are from which location tweeted here we can see that the top most hashtag are from us,london and so on."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correation=tweets.corr()\n",
        "print(correation)"
      ],
      "metadata": {
        "id": "Z68db0tCaJpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "correlation = tweets.corr()\n",
        "sns.heatmap(abs(correlation), annot= True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a correlation chart, also known as a correlation matrix, to visualize the correlation between pairs of variables in a dataset. Correlation is a statistical measure that indicates the extent to which two variables are related. A correlation value ranges from -1 to 1, where a value of 1 indicates a perfect positive correlation, a value of -1 indicates a perfect negative correlation, and a value of 0 indicates no correlation."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation matrix shows a perfect correlation between UserName and ScreenName, indicating they measure the same thing. There is a moderate positive correlation between Month and the user names/screen names, but the direction of the relationship cannot be determined."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(tweets,kind=\"scatter\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a pairplot to visualize the pairwise relationships between variables in a dataset. A pairplot is a type of scatter plot that displays the relationship between pairs of variables on a grid of subplots. In a pairplot, each variable is plotted against every other variable, producing a matrix of scatter plots."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates two visualizations for the tweets dataset: a heatmap of the absolute correlation matrix and a pairplot.the pairplot creates a grid of scatter plots for each pair of variables in the dataset. These visualizations allow us to explore and understand the relationships between variables in the tweets dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**These are the three hypothesis statements we are going to perform**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis statement 1: The number of tweets related to COVID-19 in the United States increased significantly during the months of March and April 2020.\n",
        "\n",
        "To test this hypothesis, we can perform a t-test comparing the mean number of tweets in March and April 2020 to the mean number of tweets in all other months.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between March and April 2020 and all other months.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in March and April 2020 is significantly higher than the mean number of tweets in all other months.\n",
        "\n",
        "\n",
        "Hypothesis statement 2: There is no significant difference in the number of tweets related to COVID-19 between weekdays and weekends.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets on weekdays to the mean number of tweets on weekends.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between weekdays and weekends.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States on weekends is significantly different than the mean number of tweets on weekdays.\n",
        "\n",
        "\n",
        "\n",
        "Hypothesis statement 3: The number of tweets related to COVID-19 has decreased significantly in the United States in the second half of 2020 compared to the first half of 2020.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets in the first half of 2020 to the mean number of tweets in the second half of 2020.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between the first half of 2020 and the second half of 2020.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in the second half of 2020 is significantly lower than the mean number of tweets in the first half of 2020."
      ],
      "metadata": {
        "id": "ETy7eI6MYYQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hypothetical Statement - 1**"
      ],
      "metadata": {
        "id": "HoxtOJ8sZMro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis statement 1: The number of tweets related to COVID-19 in the United States increased significantly during the months of March and April 2020.\n",
        "\n",
        "To test this hypothesis, we can perform a t-test comparing the mean number of tweets in March and April 2020 to the mean number of tweets in all other months.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between March and April 2020 and all other months.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in March and April 2020 is significantly higher than the mean number of tweets in all other months."
      ],
      "metadata": {
        "id": "d9JCRuG4Zwnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between March and April 2020 and all other months.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in March and April 2020 is significantly higher than the mean number of tweets in all other months."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "march_april = Tweet_20[(Tweet_20['TweetAt'] == '03-2020') | (Tweet_20['TweetAt'] == '04-2020')]['Tweet_Date']\n",
        "other_months = Tweet_20[(Tweet_20['TweetAt'] != '03-2020') & (Tweet_20['TweetAt'] != '04-2020')]['Tweet_Date']\n",
        "\n",
        "t_stat, p_val = stats.ttest_ind(march_april, other_months)\n",
        "\n",
        "print(\"t-statistic: \", t_stat)\n",
        "print(\"p-value: \", p_val)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this hypothesis, we can perform a t-test comparing the mean number of tweets in March and April 2020 to the mean number of tweets in all other months."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A t-test is a statistical test that is used to determine whether there is a significant difference between the means of two groups. Specifically, it tests whether the difference between the means of the two groups is statistically significant or likely due to chance.\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis statement 2: There is no significant difference in the number of tweets related to COVID-19 between weekdays and weekends.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets on weekdays to the mean number of tweets on weekends.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between weekdays and weekends.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States on weekends is significantly different than the mean number of tweets on weekdays."
      ],
      "metadata": {
        "id": "ug1aRolNZ1cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between weekdays and weekends.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States on weekends is significantly different than the mean number of tweets on weekdays."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Convert Tweet_Date column to datetime format if needed\n",
        "Tweet_20['Tweet_Date'] = pd.to_datetime(Tweet_20['Tweet_Date'])\n",
        "\n",
        "# Create a new column for numerical day of week\n",
        "Tweet_20['DayOfWeek'] = Tweet_20['Tweet_Date'].dt.weekday\n",
        "\n",
        "# Filter weekdays and weekends using the new DayOfWeek column\n",
        "weekdays = Tweet_20[Tweet_20['DayOfWeek'] < 5]['DayOfWeek']\n",
        "weekends = Tweet_20[Tweet_20['DayOfWeek'] >= 5]['DayOfWeek']\n",
        "\n",
        "# Perform t-test on the new DayOfWeek columns\n",
        "t_stat, p_val = stats.ttest_ind(weekdays, weekends)\n",
        "\n",
        "print(\"t-statistic: \", t_stat)\n",
        "print(\"p-value: \", p_val)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets on weekdays to the mean number of tweets on weekends."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " t-tests are a useful statistical tool that can help answer a wide range of questions, from whether a new product or intervention is effective to whether there are significant differences between different groups in a population."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis statement 3: The number of tweets related to COVID-19 has decreased significantly in the United States in the second half of 2020 compared to the first half of 2020.\n",
        "\n",
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets in the first half of 2020 to the mean number of tweets in the second half of 2020.\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between the first half of 2020 and the second half of 2020.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in the second half of 2020 is significantly lower than the mean number of tweets in the first half of 2020."
      ],
      "metadata": {
        "id": "Gs50sVaTaBPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: There is no significant difference in the mean number of tweets related to COVID-19 in the United States between the first half of 2020 and the second half of 2020.\n",
        "\n",
        "Alternative Hypothesis: The mean number of tweets related to COVID-19 in the United States in the second half of 2020 is significantly lower than the mean number of tweets in the first half of 2020."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "first_half = Tweet_20[Tweet_20['TweetAt'].isin(['01-2020', '02-2020', '03-2020', '04-2020', '05-2020', '06-2020'])]['Tweet_Date']\n",
        "second_half = Tweet_20[Tweet_20['TweetAt'].isin(['07-2020', '08-2020', '09-2020', '10-2020', '11-2020', '12-2020'])]['Tweet_Date']\n",
        "\n",
        "t_stat, p_val = stats.ttest_ind(first_half, second_half)\n",
        "\n",
        "print(\"t-statistic: \", t_stat)\n",
        "print(\"p-value: \", p_val)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this hypothesis, we can perform a two-sample t-test comparing the mean number of tweets in the first half of 2020 to the mean number of tweets in the second half of 2020."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A t-test is a statistical test that is used to determine whether there is a significant difference between the means of two groups. Specifically, it tests whether the difference between the means of the two groups is statistically significant or likely due to chance."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "sns.heatmap(tweets.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap technique used in the code you provided is called a missing value heatmap, and inside there is pandas function which is used to check the null value it is a useful way to visualize the distribution and pattern of missing values in a dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "freq = tweets['Sentiment'].value_counts().reset_index().rename(columns = {'index':'Sentiment','Sentiment':'Counts'})\n",
        "freq"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.boxplot(y=freq['Counts'])\n"
      ],
      "metadata": {
        "id": "Mr_ViNgkrnU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is a visualization technique used to display the distribution of a dataset. It shows the range, median, quartiles, and potential outliers of the data."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform the 'Sentiment' column\n",
        "tweets['Sentiment_Encoded'] = le.fit_transform(tweets['Sentiment'])\n",
        "\n",
        "# display the updated dataframe\n",
        "print(tweets)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we create a LabelEncoder object and use it to transform the 'Sentiment' column into numerical values. The transformed values are then added as a new column 'Sentiment_Encoded' to the dataframe. We can do the same for other categorical columns in the dataset as well."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "import contractions\n",
        "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(lambda x: contractions.fix(x))\n",
        "print(tweets.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "tweets[\"OriginalTweet\"] = tweets[\"OriginalTweet\"].str.lower()\n",
        "tweets['OriginalTweet']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Writing a function which removes punctuation from our dataset\n",
        "def remove_punc(text):\n",
        "  no_punc = [char for char in text if char not in string.punctuation]\n",
        "  return ''.join(no_punc)\n",
        "     \n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating another column TokenizedTweet and store the Clean punctuation\n",
        "tweets['TokenizedTweet'] = tweets['OriginalTweet'].apply(remove_punc)"
      ],
      "metadata": {
        "id": "H4ObphYBpouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.head()"
      ],
      "metadata": {
        "id": "3ceLzy2xpuqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# Remove URLs from the OriginalTweet column\n",
        "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "\n",
        "# Remove words and digits containing digits from the OriginalTweet column\n",
        "tweets['OriginalTweet'] = tweets['OriginalTweet'].apply(lambda x: ' '.join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n",
        "\n",
        "# Display the cleaned dataset\n",
        "print(tweets.head())\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "# Writing a function which removes stopwords from our data set\n",
        "stop_list=list(stopwords.words('english'))\n",
        "def remove_stopwords(msg):\n",
        "    no_stop_words = [word for word in msg.split() if word.lower() not in stop_list]\n",
        "    return ' '.join(no_stop_words)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['TokenizedTweet'] = tweets['TokenizedTweet'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "u9WN02Klx_gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "# Remove white spaces from the OriginalTweet column\n",
        "tweets['OriginalTweet'] = tweets['OriginalTweet'].str.replace(' ', '')\n",
        "\n",
        "# Display the cleaned dataset\n",
        "print(tweets.head())"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Define a dictionary to map the sentiment values to their corresponding phrases\n",
        "sentiment_phrases = {'Neutral': 'This tweet has a neutral sentiment.',\n",
        "                     'Positive': 'This tweet has a positive sentiment.',\n",
        "                     'Negative': 'This tweet has a negative sentiment.',\n",
        "                     'Extremely Positive': 'This tweet has an extremely positive sentiment.',\n",
        "                     'Extremely Negative': 'This tweet has an extremely negative sentiment.'}\n",
        "\n",
        "# Use the map() function to replace the sentiment values with their corresponding phrases\n",
        "tweets['Sentiment'] = tweets['Sentiment'].map(sentiment_phrases)\n",
        "\n",
        "# Print the updated dataframe\n",
        "print(tweets.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "sirCT3GTHEZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Tokenize the OriginalTweet column\n",
        "tweets['TokenizedTweet'] = tweets['OriginalTweet'].apply(lambda x: nltk.word_tokenize(x))\n",
        "\n",
        "# Display the tokenized dataset\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Words are cropped to save space. SnowballStemmer will be used for that.**"
      ],
      "metadata": {
        "id": "tuCXPgebyU2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(words):\n",
        "    stemmed_words = []\n",
        "    for word in words:\n",
        "        stemmed_words.append(stemmer.stem(word))\n",
        "    return stemmed_words\n",
        "    "
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['TokenizedTweet'] = tweets['TokenizedTweet'].apply(stemming)"
      ],
      "metadata": {
        "id": "MPHsOvgIzeAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "tweets['TokenizedTweet'] = tweets['OriginalTweet'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
      ],
      "metadata": {
        "id": "KmH8D2Mrzt9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.head()"
      ],
      "metadata": {
        "id": "evtMFbCr1aas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above shows the use of two text normalization techniques - stemming and lemmatization.\n",
        "\n",
        "Stemming is the process of reducing a word to its base or root form. It involves removing the suffixes from words so that words with the same base form can be grouped together. The SnowballStemmer from the Natural Language Toolkit (NLTK) library is used in the code to stem the words in the text.\n",
        "\n",
        "Lemmatization is another text normalization technique that involves reducing words to their base or root form, but it takes into account the context of the word and uses a dictionary to match the word to its root form. The WordNetLemmatizer from the NLTK library is used in the code to lemmatize the words in the text."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# Download the required nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# Define a function to perform POS tagging on a given text\n",
        "def pos_tag(text):\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    return tagged_tokens\n",
        "\n",
        "# Perform POS tagging on the OriginalTweet column\n",
        "tweets['PosTaggedTweet'] = tweets['OriginalTweet'].apply(pos_tag)\n",
        "\n",
        "# Display the POS-tagged dataset\n",
        "print(tweets.head())"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.head()"
      ],
      "metadata": {
        "id": "P5_XDlHFHVhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Define a function to tokenize the tweets\n",
        "def tokenize_tweet(tweet):\n",
        "    return word_tokenize(tweet.lower())\n",
        "\n",
        "# Tokenize the OriginalTweet column using the tokenize_tweet() function\n",
        "tweets['TokenizedTweet'] = tweets['OriginalTweet'].apply(tokenize_tweet)\n",
        "\n",
        "# Join the tokens in each tweet to form a single string\n",
        "tweets['TweetString'] = tweets['TokenizedTweet'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Vectorize the TweetString column using CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "count_vectors = cv.fit_transform(tweets['TweetString'])\n",
        "\n",
        "# Vectorize the TweetString column using TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf.fit_transform(tweets['TweetString'])\n",
        "\n",
        "# Print the dimensions of the vectorized data\n",
        "print('CountVectorizer output shape:', count_vectors.shape)\n",
        "print('TfidfVectorizer output shape:', tfidf_vectors.shape)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer (short for term frequency-inverse document frequency) is a more advanced technique that also takes into account the importance or rarity of words in a document. In addition to counting the frequency of words, TfidfVectorizer also assigns a weight to each word based on how frequently it appears in the corpus and how often it appears in a specific document relative to its appearance in other documents. This technique can be useful for capturing the unique features of a document and identifying words that are important to that document specifically."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Create new feature \"TweetLength\"\n",
        "tweets['TweetLength'] = tweets['OriginalTweet'].apply(lambda x: len(x))\n",
        "\n",
        "# Extract month from \"TweetAt\" column and create new feature \"Month\"\n",
        "tweets['Month'] = pd.to_datetime(tweets['TweetAt'], format='%d-%m-%Y').dt.month\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = tweets.corr()\n",
        "\n",
        "# Identify highly correlated features\n",
        "high_corr_features = set()\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "# Remove highly correlated features\n",
        "tweets.drop(high_corr_features, axis=1, inplace=True)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(tweets.head())\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Create new feature \"TweetLength\"\n",
        "tweets['TweetLength'] = tweets['OriginalTweet'].apply(lambda x: len(x))\n",
        "\n",
        "# Extract month from \"TweetAt\" column and create new feature \"Month\"\n",
        "tweets['Month'] = pd.to_datetime(tweets['TweetAt'], format='%d-%m-%Y').dt.month.astype(int)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = tweets.corr()\n",
        "\n",
        "# Identify highly correlated features\n",
        "high_corr_features = set()\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "# Remove highly correlated features\n",
        "tweets.drop(high_corr_features, axis=1, inplace=True)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(tweets.head())\n"
      ],
      "metadata": {
        "id": "AVqN94rVOpJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate Feature Selection and Recursive Feature Elimination.\n",
        "Univariate Feature Selection is a statistical technique that involves testing each feature individually to determine its relationship with the target variable.\n",
        "\n",
        "Recursive Feature Elimination (RFE) is a machine learning technique that involves fitting a model and recursively removing the least important features until a specified number of features remain"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate Feature Selection and Recursive Feature Elimination.\n",
        "\n",
        "Univariate Feature Selection is a statistical technique that involves testing each feature individually to determine its relationship with the target variable. In the code provided, SelectKBest is used to select the k best features based on the chi-squared test statistic. This method is useful for quickly identifying the features that have the strongest relationship with the target variable, and can help reduce the number of features in the dataset to improve model performance and reduce overfitting.\n",
        "\n",
        "Recursive Feature Elimination (RFE) is a machine learning technique that involves fitting a model and recursively removing the least important features until a specified number of features remain. In the code provided, a RandomForestClassifier is used to fit the model and RFE is used to select the top 10 features that contribute most to the model's accuracy. This method is useful when the relationship between the features and target variable is complex, and when the goal is to identify the features that are most important for prediction."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "#no we don't have so much \n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# create a label encoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# encode the non-numeric values in the 'Sentiment' column as integers\n",
        "tweets['Sentiment_Encoded'] = label_encoder.fit_transform(tweets['Sentiment'])\n",
        "# select the columns to scale\n",
        "columns_to_scale = ['Sentiment_Encoded']\n",
        "\n",
        "# create a scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit the scaler to the selected columns\n",
        "scaler.fit(tweets[columns_to_scale])\n",
        "\n",
        "# transform the selected columns using the scaler\n",
        "tweets[columns_to_scale] = scaler.transform(tweets[columns_to_scale])\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[columns_to_scale]"
      ],
      "metadata": {
        "id": "Y9Wi4QibBYVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinMaxScaler is that it scales the data to a fixed range (by default, the range is 0 to 1), which is suitable for many machine learning algorithms that expect input data to be on a similar scale. Additionally, MinMaxScaler is robust to small outliers and can handle negative values."
      ],
      "metadata": {
        "id": "rYKkZYYMgT-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No we don't use the dimensionality reduction here because we have limited features here\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining independent variables\n",
        "x = tweets['TokenizedTweet']\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining dependent variable\n",
        "y = tweets['Sentiment']"
      ],
      "metadata": {
        "id": "X3ST1xtX1CaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) \n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "UCzupt9_1Fuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "zXM22nFwG6oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data splitting ratio used in this code is 80:20, which means that 80% of the data is used for training and 20% of the data is used for testing. This is a common splitting ratio used in machine learning tasks as it ensures that the model is trained on a sufficiently large amount of data and tested on a separate, unseen dataset. The random_state parameter is set to 0 to ensure reproducibility of the split."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Vectorization**\n",
        "**Creating an object of TfidfVectorizer, the test data was normalised, and stored in the variables X_test and X_train, and also both predicting actual and predicted values.**"
      ],
      "metadata": {
        "id": "v5uRCyQrl-vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the inner lists into strings\n",
        "X_train = [' '.join(x) for x in X_train]\n",
        "X_test = [' '.join(x) for x in X_test]\n",
        "\n",
        "# Creating an object of TfidfVectorizer\n",
        "tf_idf = TfidfVectorizer()\n",
        "\n",
        "# Applying tf idf to training and test data\n",
        "X_train_tf = tf_idf.fit_transform(X_train)\n",
        "X_test_tf = tf_idf.transform(X_test)\n",
        "\n",
        "# Print the variables\n",
        "print(X_train_tf.shape)\n",
        "print(X_test_tf.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "     \n",
        "     "
      ],
      "metadata": {
        "id": "705vPj3At-p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No this dataset is not imbalanace we don't have to much feature."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Logistic Regression**"
      ],
      "metadata": {
        "id": "_bSv26qY4rMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# creating an object of LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_reg = LogisticRegression()\n",
        "# Here, we fit the model, predicting on test set and finding the evaluation metrics for a model.\n",
        "# Fit the Algorithm\n",
        "logistic_reg.fit(X_train_tf, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "logistic_reg_prediction = logistic_reg.predict(X_test_tf)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Finding the accuracy_score & evaluation metrics for a model\n",
        "logreg_accuracy = accuracy_score(y_test,logistic_reg_prediction)\n",
        "print(\"Training accuracy Score : \",logistic_reg.score(X_train_tf,y_train).round(4))\n",
        "print(\"Testing accuracy Score : \",logreg_accuracy.round(4))\n",
        "print(classification_report(logistic_reg_prediction,y_test))\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create a logistic regression object\n",
        "logistic_reg = LogisticRegression()\n",
        "\n",
        "# Set the parameter grid to search over\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Create a GridSearchCV object and fit it to the data\n",
        "grid_search = GridSearchCV(logistic_reg, param_grid, cv=5)\n",
        "grid_search.fit(X_train_tf, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding score\n",
        "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n",
        "\n",
        "# Use the best hyperparameters to create the final model\n",
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Predict on the test set using the final model\n",
        "final_prediction = final_model.predict(X_test_tf)\n",
        "\n",
        "# Find the accuracy score and evaluation metrics for the final model\n",
        "final_accuracy = accuracy_score(y_test, final_prediction)\n",
        "print(\"Final Training accuracy Score : \", final_model.score(X_train_tf,y_train).round(4))\n",
        "print(\"Final Testing accuracy Score : \", final_accuracy.round(4))\n",
        "print(classification_report(final_prediction, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used the GridSearch CV for hyperparameter optimization. GridSearch CV is a commonly used technique for hyperparameter optimization because it exhaustively searches over a grid of hyperparameters and evaluates each combination of hyperparameters using cross-validation. This approach is easy to implement and can be effective for finding good hyperparameters for a given model. However, it can be computationally expensive, especially when there are many hyperparameters or the dataset is large. Other techniques like RandomSearch CV and Bayesian optimization can be more efficient in such cases.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the hyperparameter tuning was performed using GridSearchCV to find the best hyperparameters for the logistic regression model. The code also calculates the accuracy and classification report of the tuned model. If the hyperparameters selected by GridSearchCV result in a better model, we can expect to see an improvement in the evaluation metric scores."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# creating an object of RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "Randm_Forest_Clf = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Here, we fit the model, predicting on test set and finding the evaluation metrics for a model.\n",
        "Randm_Forest_Clf.fit(X_train_tf,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "# Predicting test model\n",
        "Randm_Forest_prediction = Randm_Forest_Clf.predict(X_test_tf)"
      ],
      "metadata": {
        "id": "qg0xk6yf0abn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Finding the accuracy_score & evaluation metrics for a model\n",
        "Randm_Forest_accuracy = accuracy_score(y_test,Randm_Forest_prediction)\n",
        "print(\"Training accuracy Score : \",Randm_Forest_Clf.score(X_train_tf,y_train))\n",
        "print(\"Testing accuracy Score : \",Randm_Forest_accuracy)\n",
        "print(classification_report(Randm_Forest_prediction,y_test))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier object\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train_tf, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Predict on the model\n",
        "# Predict on the model using the best hyperparameters\n",
        "best_clf = grid_search.best_estimator_\n",
        "best_prediction = best_clf.predict(X_test_tf)\n",
        "\n",
        "# Evaluate the performance of the model using accuracy score and classification report\n",
        "best_accuracy = accuracy_score(y_test, best_prediction)\n",
        "print(\"Training accuracy Score : \", best_clf.score(X_train_tf, y_train))\n",
        "print(\"Testing accuracy Score : \", best_accuracy)\n",
        "print(classification_report(best_prediction, y_test))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we uses GridSearchCV as a hyperparameter optimization technique. GridSearchCV exhaustively searches over a specified parameter grid and returns the best hyperparameters for a given model. It is commonly used when you have a small number of hyperparameters to tune and want to search over all possible combinations of hyperparameter values. Other hyperparameter optimization techniques such as RandomizedSearchCV or Bayesian Optimization can also be used depending on the problem and nature of the hyperparameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I cannot provide an evaluation metric score chart without the initial performance metrics of the model before the hyperparameter optimization. However, by using the GridSearchCV technique to tune the hyperparameters of the Random Forest Classifier, it is expected that the model's performance has improved compared to the default settings. The best hyperparameters found by the GridSearchCV method should have increased the accuracy score and potentially other evaluation metrics."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics such as accuracy, precision, recall, F1-score, and AUC-ROC provide insights into the performance of an ML model. These metrics can help businesses make data-driven decisions and optimize their operations, leading to cost reduction, increased efficiency, and improved customer satisfaction."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "#Creating an object of  Naive Bayes classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomial_naive_bayes = MultinomialNB()\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fitting Naive Bayes classifier \n",
        "multinomial_naive_bayes.fit(X_train_tf,y_train)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Predicting test model\n",
        "multinomial_naive_bayes_prediction = multinomial_naive_bayes.predict(X_test_tf)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Finding the accuracy_score & evaluation metrics for a model\n",
        "multinomial_naive_bayes_accuracy = accuracy_score(y_test,multinomial_naive_bayes_prediction)\n",
        "print(\"Training accuracy Score : \",multinomial_naive_bayes.score(X_train_tf,y_train))\n",
        "print(\"Testing accuracy Score : \",multinomial_naive_bayes_accuracy)\n",
        "print(classification_report(multinomial_naive_bayes_prediction,y_test))\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set up the hyperparameter grid\n",
        "hyperparameters = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
        "}\n",
        "\n",
        "# Create a new Naive Bayes classifier\n",
        "multinomial_naive_bayes = MultinomialNB()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(multinomial_naive_bayes, hyperparameters, cv=5)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train_tf, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "# Use the best model to make predictions on the test data\n",
        "best_model = grid_search.best_estimator_\n",
        "multinomial_naive_bayes_prediction = best_model.predict(X_test_tf)\n",
        "\n",
        "# Finding the accuracy_score & evaluation metrics for the best model\n",
        "multinomial_naive_bayes_accuracy = accuracy_score(y_test, multinomial_naive_bayes_prediction)\n",
        "print(\"Training accuracy Score : \", best_model.score(X_train_tf, y_train))\n",
        "print(\"Testing accuracy Score : \", multinomial_naive_bayes_accuracy)\n",
        "print(classification_report(multinomial_naive_bayes_prediction, y_test))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform hyperparameter tuning, we can use GridSearchCV or RandomizedSearchCV from sklearn's model_selection module. Both techniques search over a grid of hyperparameters to find the best combination for the model. GridSearchCV performs an exhaustive search over the entire grid, while RandomizedSearchCV samples randomly from the grid. The choice of which technique to use depends on the size of the hyperparameter space and the computational resources available."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the given code does not include hyperparameter optimization, I cannot provide the improvement in the evaluation metric score chart. However, if we were to implement hyperparameter tuning using GridSearchCV or RandomizedSearchCV, we may be able to improve the performance of the Naive Bayes classifier. By finding the best combination of hyperparameters, we may see an improvement in accuracy, precision, recall, and F1 score, which would be reflected in the updated evaluation metric score chart."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of evaluation metrics for a positive business impact depends on the specific goals and requirements of the problem. Common metrics for classification problems include accuracy, precision, recall, F1-score, ROC-AUC, and log-loss. Metrics should be chosen to reflect business objectives and communicated transparently to all stakeholders. The choice of metrics should be based on a thorough understanding of the problem and data, and should be continuously monitored and updated as the business environment evolves."
      ],
      "metadata": {
        "id": "WtCTwq1_i3v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the final prediction model depends on the dataset size, business requirements, computational resources, and desired model complexity. Simpler models like Naive Bayes or logistic regression may be preferred for smaller datasets and interpretability, while complex models like decision trees or neural networks may be better for larger datasets or complex relationships. The final choice involves balancing accuracy, interpretability, and computational cost through experimentation and evaluation."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a probabilistic classification algorithm that assumes features are independent given the class label. It can work well in many cases and is a simple and efficient algorithm. The model computes the posterior probability of each class label for a given input and chooses the label with the highest posterior probability. The model does not provide a direct measure of feature importance or feature effects on the target variable."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The majority of the tweets were around 250 characters long, indicating that there was a lot of interest in COVID-19 among the general public.\n",
        "\n",
        "2.More positive tweets than neutral or negative ones were tweeted globally.\n",
        "\n",
        "3.People tweeted more in March than in April since many nations imposed lockdown during this time.\n",
        "\n",
        "4.The United States and London (England) were the two countries with the most tweets.\n",
        "\n",
        "5.We saw inconsistent responses from Australia during the pandemic, with nearly equal numbers of positive and negative tweets.\n",
        "\n",
        "6.Words like COVID19, grocery, supermarket, shop, price, etc. are frequently used in tweets, indicating that throughout the pandemic, individuals were mostly concerned about food supply and their costs.\n",
        "\n",
        "7.Support Vector Classifier has performed slightly better than the Logistic regression and got the highest test accuracy score around 60%.\n",
        "\n",
        "8.Multinomial Naive Bayes performed the worst with test accuracy score of just 0.35."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}